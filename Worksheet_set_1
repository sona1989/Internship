# Internship
Flip Robo worksheet 1 
 
PYTHON WORKSHEET 1 

Python worksheet 1 

1. C - % ans 
2. B. 0  ans
3. 6<<2  = 24 ans
4. 6&2   - 2 ANS
     6 - 110
     2-  010
     ‐----------
     010 - 2 ans 

5. 6|2 - 110 - 6 ans 
6.  C ans
7. A 
8. C in defining a generator 
9. A and C 
10. A and B

11. To find factorial of number 

# Python program to find the factorial of a number provided by the user.

 # change the value for a different result num = 7

 # To take input from the user 
num = int(input("Enter a number: "))
 factorial = 1

 # check if the number is negative, positive or zero

 if num < 0: 
    print("Sorry, factorial does not exist for negative numbers")

 elif num == 0:
    print("The factorial of 0 is 1")

 else: 
        for i in range(1,num + 1):
        factorial = factorial*i
 print("The factorial of",num,"is",factorial)

Using recursion

def recur_factorial(x): 
            if x == 1:
                 return 1
           else: 
                return (x * recur_factorial(x - 1))

 num = int(input("Enter number: ")) 
print("The factorial of", num, "is", recur_factorial(num))


‐--------------------------------------------------
12. PRIME OR NOT PRIME PROGRAM 

 #Program :

num = int(input("Enter any number : "))

 if num > 1:
           for i in range(2, num): 
            if (num % i) == 0:
 print(num, "is NOT a prime number")
             break
           else:
 print(num, "is a PRIME number") 
           elif num == 0 or 1: 
print(num, "is a neither prime NOR composite number") 
           else: 
print(num, "is NOT a prime number it is a COMPOSITE number")

---------------------------------------------------------------
13. # Python program to check

# if a string is palindrome 

# or not

 

x = "malayalam" 

 

w = "" 

for i in x: 

    w = i + w 

 

if (x == w): 

    print("Yes") 

else: 

    print("No") 


Or 

# function which return reverse of a string

 

def isPalindrome(s): 

    return s == s[::-1] 

 

 

# Driver code

s = "malayalam" 

ans = isPalindrome(s) 

 

if ans: 

    print("Yes") 

else: 

    print("No")

-----------------------------------------------
14. 

def pythagoras(opposite_side, adjacent_side,hypotenuse): 
    if opposite_side == str("x"): 
         return ("Opposite = " +  str(((hypo tenuse**2) - (adjacent_side**2))**0.5)) 
    elif adjacent_side == str("x"):
         return ("Adjacent = " + str(((hypo tenuse**2) - (opposite_side**2))**0.5)) 
    elif hypotenuse == str("x"):
          return ("Hypotenuse = " + str (((opposite_side**2) + (adjacent_side **2))**0.5)) 
else: return "You know the answer!"

 print(pythagoras(3,4,'x')) print(pythagoras(3,'x',5)) print(pythagoras('x',4,5)) print(pythagoras(3,4,5)) 



Sample Output:

Hypotenuse = 5.0 
Adjacent = 4.0 
Opposite = 3.0
 You know the answer! 
-‐--------------------------
15. TO KNOW FREQUENCY 

# Python3 code to demonstrate  

# each occurrence frequency using  

# naive method  

  

# initializing string  

test_str = "GeeksforGeeks" 

  

# using naive method to get count  

# of each element in string  

all_freq = {} 

  

for i in test_str: 

    if i in all_freq: 

        all_freq[i] += 1 

    else: 

        all_freq[i] = 1

# printing result  

print ("Count of all characters in GeeksforGeeks is :\n " 

 ----------------------------

Statistics worksheet 1 

Statistics

1. TRUE

2. A

3. D

4. D
5. C
6. A
7. B
8. A
9. C
10 . In probability theory, a normal (or Gaussian or Gauss or Laplace–Gauss) distribution is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is
Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known.[2][3] Their importance is partly due to the central limit theorem. It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable—whose distribution converges to a normal distribution as the number of samples increases. Therefore, physical quantities that are expected to be the sum of many independent processes, such as measurement errors, often have distributions that are nearly normal.[4]

Moreover, Gaussian distributions have some unique properties that are valuable in analytic studies. For instance, any linear combination of a fixed collection of normal deviates is a normal deviate. Many results and methods, such as propagation of uncertainty and least squares parameter fitting, can be derived analytically in explicit form when the relevant variables are normally distributed.

A normal distribution is sometimes informally called a bell curve.[5] However, many other distributions are bell-shaped (such as the Cauchy, Student's t, and logistic distributions).

----------
11.
3 Methods to Handle Missing Data

Types of Missing Data

Understanding the nature of missing data is critical in determining what treatments can be applied to overcome the lack of data. Data can be missing in the following ways:

• Missing Completely At Random (MCAR): When missing values are randomly distributed across all observations, then we consider the data to be missing completely at random. A quick check for this is to compare two parts of data – one with missing observations and the other without missing observations. On a t-test, if we do not find any difference in means between the two samples of data, we can assume the data to be MCAR.

• Missing At Random (MAR): The key difference between MCAR and MAR is that under MAR the data is not missing randomly across all observations, but is missing randomly only within sub-samples of data. For example, if high school GPA data is missing randomly across all schools in a district, that data will be considered MCAR. However, if data is randomly missing for students in specific schools of the district, then the data is MAR.

• Not Missing At Random (NMAR): When the missing data has a structure to it, we cannot treat it as missing at random. In the above example, if the data was missing for all students from specific schools, then the data cannot be treated as MAR.

Common Methods

1. Mean or Median Imputation

When data is missing at random, we can use list-wise or pair-wise deletion of the missing observations. However, there can be multiple reasons why this may not be the most feasible option:

• There may not be enough observations with non-missing data to produce a reliable analysis

• In predictive analytics, missing data can prevent the predictions for those observations which have missing data

• External factors may require specific observations to be part of the analysis

In such cases, we impute values for missing data. A common technique is to use the mean or median of the non-missing observations. This can be useful in cases where the number of missing observations is low. However, for large number of missing values, using mean or median can result in loss of variation in data and it is better to use imputations. Depending upon the nature of the missing data, we use different techniques to impute data that have been described below.

2. Multivariate Imputation by Chained Equations (MICE)

MICE assumes that the missing data are Missing at Random (MAR). It imputes data on a variable-by-variable basis by specifying an imputation model per variable. MICE uses predictive mean matching (PMM) for continuous variables, logistic regressions for binary variables, bayesian polytomous regressions for factor variables, and proportional odds model for ordered variables to impute missing data.

To set up the data for MICE, it is important to note that the algorithm uses all the variables in the data for predictions. In this case, variables that may not be useful for predictions, like the ID variable, should be removed before implementing this algorithm.
<span class="hs_cos_wrapper hs_cos_wrapper_meta_field hs_cos_wrapper_type_rich_text" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text" id="hs_cos_wrapper_post_body" style=""><code class="language-R">Data$ID <- NULL</code></span>

Secondly, as mentioned above, the algorithm treats different variables differently. So, all categorical variables should be treated as factor variables before implementing MICE.

<span class="hs_cos_wrapper hs_cos_wrapper_meta_field hs_cos_wrapper_type_rich_text" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text" id="hs_cos_wrapper_post_body" style=""><code class="language-R">Data$year <- as.factor(Data$year) Data$gender <- as.factor(Data$gender)</code></span>

Then you can implement the algorithm using the MICE library in R


<span class="hs_cos_wrapper hs_cos_wrapper_meta_field hs_cos_wrapper_type_rich_text" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text" id="hs_cos_wrapper_post_body" style=""><code class="language-R">library(mice) init = mice(Data, maxit=0) method = init$method predMat = init$predictorMatrix set.seed(101) imputed = mice(Data, method=method, predictorMatrix=predMat, m=5)</code></span>

You can also ignore some variables as predictors or skip a variable from being imputed using the MICE library in R. Additionally, the library also allows you to set a method of imputation discussed above depending upon the nature of the variable.

3. Random Forest

Random forest is a non-parametric imputation method applicable to various variable types that works well with both data missing at random and not missing at random. Random forest uses multiple decision trees to estimate missing values and outputs OOB (out of bag) imputation error estimates. 

One caveat is that random forest works best with large datasets and using random forest on small datasets runs the risk of overfitting. The extent of overfitting leading to inaccurate imputations will depend upon how closely the distribution for predictor variables for non-missing data resembles the distribution of predictor variables for missing data. For example, if the distribution of race/ethnicity for non-missing data is similar to the distribution of race/ethnicity for missing data, overfitting is not likely to throw off results. However, if the two distributions differ, the accuracy of imputations will suffer.

The MICE library in R also allows imputations by random forest by setting the method to “rf”. The authors of the MICE library have provided an example on how to implement the random forest method here.

To sum up data imputations is tricky and should be done with care. It is important to understand the nature of the data that is missing when deciding which algorithm to use for imputations. While using the above algorithms, predictor variables should be set up carefully to avoid confusion in the methods implemented during imputation. Finally, you can test the quality of your imputations by normalized root mean square error (NRMSE) for continuous variables and proportion of falsely classified (PFC) for categorical variables.


• Ignore the records with missing values.

Many tools ignore records with missing values. When the percentage of records with missing values is small, we could ignore those records.

• Substitute a value such as mean.

When the percentage is large and also when it makes sense to do something to avoid bias modeling results, substituting a value (e.g. mean, median) is a commonly used way. But this method could cause bias distribution and variance. That’s where the following imputation methods come in.

• Predict missing values.

Depending on the type of the imputed variable (i.e. continuous, ordinal, nominal) and missing data pattern (i.e. monotone, non-monotone), below are a few commonly used models. If you plan to do it in SAS, there are SAS codes that you can write to identify the missing data pattern.

• Logistic Regression

• Discriminant Regression

• Markov Chain Monte Carlo (MCMC)

• …

• Predict missing values - Multiple Imputation. Although there are pros & cons, MI is considered to be superior to single imputation, and it better measures the uncertainty of the missing values.

In addition, there are a few required statistical assumptions for multiple imputation:

• Whether the data is missing at random (MAR).

• Multivariate normal distribution, for some of the modeling methods mentioned above (e.g. regression, MCMC).

• …

At last, if you have to think of what to report -

• The type of imputation algorithm used.

• Some justification for choosing a particular imputation method.

• The proportion of missing observations.

• The number of imputed datasets (m) created.

• The variables used in the imputation model.

---------
12.
A/B testing (also known as bucket testing or split-run testing) is a user experience research methodology.[1] A/B tests consist of a randomized experiment with two variants, A and B.[2][3][4] It includes application of statistical hypothesis testing or "two-sample hypothesis testing" as used in the field of statistics. A/B testing is a way to compare two versions of a single variable, typically by testing a subject's response to variant A against variant B, and determining which of the two variants is more effective.

A/B testing is a shorthand for a simple randomized controlled experiment, in which two samples (A and B) of a single vector-variable are compared.[1] These values are similar except for one variation which might affect a user's behavior. A/B tests are widely considered the simplest form of controlled experiment. However, by adding more variants to the test, its complexity grows.[6]

A/B tests are useful for understanding user engagement and satisfaction of online features like a new feature or product.[7] Large social media sites like LinkedIn, Facebook, and Instagram use A/B testing to make user experiences more successful and as a way to streamline their services.[7]

Today, A/B tests are being used also for conducting complex experiments on subjects such as network effects when users are offline, how online services affect user actions, and how users influence one another.[7] Many professions use the data from A/B tests. This includes data engineers, marketers, designers, software engineers, and entrepreneurs.[8] Many positions rely on the data from A/B tests, as they allow companies to understand growth, increase revenue, and optimize customer satisfaction.[8]

Version A might be a version used at present (thus forming the control group), while version B is modified in some respect vs. A (thus forming the treatment group). For instance, on an e-commerce website the purchase funnel is typically a good candidate for A/B testing, since even marginal-decreases in drop-off rates can represent a significant gain in sales. Significant improvements can be sometimes seen through testing elements like copy text, layouts, images and colors,[9] but not always. In these tests, users only see one of two versions, since the goal is to discover which of the two versions is preferable.[10]

Multivariate testing or multinomial testing is similar to A/B testing, but may test more than two versions at the same time or use more controls. Simple A/B tests are not valid for observational, quasi-experimental or other non-experimental situations - commonplace with survey data, offline data, and other, more complex phenomena.

A/B testing is claimed by some to be a change in philosophy and business-strategy in certain niches, though the approach is identical to a between-subjects design, which is commonly used in a variety of research traditions.[11][12][13] A/B testing as a philosophy of web development brings the field into line with a broader movement toward evidence-based practice. The benefits of A/B testing are considered to be that it can be performed continuously on almost anything, especially since most marketing automation software now typically comes with the ability to run A/B tests on an ongoing basis.

--------
13.
True, imputing the mean preserves the mean of the observed data. So if the data are missing completely at random, the estimate of the mean remains unbiased. ... Since most research studies are interested in the relationship among variables, mean imputation is not a good solution.

----
14. 
In statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.[2]

In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models.[3] Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.

Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications.[4] This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.

Linear regression has many practical uses. Most applications fall into one of the following two broad categories:

• If the goal is prediction, forecasting, or error reduction,[clarification needed] linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.

• If the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response.

Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the "lack of fit" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms "least squares" and "linear model" are closely linked, they are not synonymous.

--------
15. 
Descriptive Statistics and Inferential Statistics

Descriptive Statistics

Descriptive statistics deals with the presentation and collection of data. This is usually the first part of a statistical analysis. It is usually not as simple as it sounds, and the statistician needs to be aware of designing experiments, choosing the right focus group and avoid biases that are so easy to creep into the experiment.

Different areas of study require different kinds of analysis using descriptive statistics. For example, a physicist studying turbulence in the laboratory needs the average quantities that vary over small intervals of time. The nature of this problem requires that physical quantities be averaged from a host of data collected through the experiment.


Inferential Statistics

Inferential statistics, as the name suggests, involves drawing the right conclusions from the statistical analysis that has been performed using descriptive statistics. In the end, it is the inferences that make studies important and this aspect is dealt with in inferential statistics.

Most predictions of the future and generalizations about a population by studying a smaller sample come under the purview of inferential statistics. Most social sciences experiments deal with studying a small sample population that helps determine how the population in general behaves. By designing the right experiment, the researcher is able to draw conclusions relevant to his study.

While drawing conclusions, one needs to be very careful so as not to draw the wrong or biased conclusions. Even though this appears like a science, there are ways in which one can manipulate studies and results through various means. For example, data dredging is increasingly becoming a problem as computers hold loads of information and it is easy, either intentionally or unintentionally, to use the wrong inferential methods.

Both descriptive and inferential statistics go hand in hand and one cannot exist without the other. Good scientific methodology needs to be followed in both these steps of statistical analysis and both these branches of statistics are equally important for a researcher.













